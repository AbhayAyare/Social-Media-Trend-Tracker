# ğŸ“Š Social Media Trend Tracker

A comprehensive data analytics project that analyzes social media trends, sentiment, and emerging topics using **NLP** and **machine learning**. Track Twitter and Reddit data to gain real-time insights into public opinion and trending topics.

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![Twitter API](https://img.shields.io/badge/Twitter-API%20v2-blue)
![Streamlit](https://img.shields.io/badge/Dashboard-Streamlit-red)
![NLP](https://img.shields.io/badge/Analysis-NLP%2BML-orange)

ğŸ¥ Video Demonstration
Watch the complete project walkthrough and setup guide:

Recording 2025-11-14 235757.mp4
Click above to watch the complete project demonstration

## ğŸ¯ Project Overview

This project tracks and analyzes social media trends from Twitter and Reddit to:

- ğŸ” **Identify emerging topics** and themes in real-time
- â¤ï¸ **Analyze public sentiment** and emotional trends over time  
- ğŸ“ˆ **Visualize trend evolution** with interactive dashboards
- ğŸš¨ **Detect viral content** and predict trending topics
- ğŸ“Š **Provide actionable insights** for businesses and researchers

## ğŸ—ï¸ Project Architecture

```
Data Sources â†’ Collection â†’ Processing â†’ Analysis â†’ Visualization â†’ Insights
    â†“            â†“           â†“           â†“           â†“             â†“
  Twitter     Tweepy     Cleaning    Sentiment   Streamlit    Trend Reports
   Reddit     API/PS     Pipeline    Topic ML    Dashboard    Alert System
```

## ğŸ“ Project Structure

```
social_media_trend_tracker/
â”‚
â”œâ”€â”€ data/                          # Data storage
â”‚   â”œâ”€â”€ raw/                       # Raw collected data
â”‚   â””â”€â”€ processed/                 # Cleaned and processed data
â”‚
â”œâ”€â”€ notebooks/                     # Jupyter notebooks for analysis
â”‚   â”œâ”€â”€ 01_data_collection.ipynb
â”‚   â”œâ”€â”€ 02_data_cleaning.ipynb
â”‚   â”œâ”€â”€ 03_nlp_analysis.ipynb
â”‚   â””â”€â”€ 04_visualization.ipynb
â”‚
â”œâ”€â”€ src/                           # Source code modules
â”‚   â”œâ”€â”€ data_collection.py        # Twitter/Reddit data collection
â”‚   â”œâ”€â”€ data_cleaning.py          # Text preprocessing pipeline
â”‚   â”œâ”€â”€ nlp_analysis.py           # Sentiment & topic modeling
â”‚   â””â”€â”€ visualization.py          # Plotting and charts
â”‚
â”œâ”€â”€ dashboard/                     # Interactive web dashboard
â”‚   â”œâ”€â”€ app.py                    # Streamlit main application
â”‚   â””â”€â”€ requirements.txt          # Dashboard dependencies
â”‚
â”œâ”€â”€ config/                        # Configuration files
â”‚   â”œâ”€â”€ settings.py               # Project settings
â”‚   â””â”€â”€ twitter_config.ini        # API credentials template
â”‚
â”œâ”€â”€ results/                       # Generated outputs
â”‚   â”œâ”€â”€ charts/                   # Saved visualizations
â”‚   â””â”€â”€ reports/                  # Analysis reports
â”‚
â”œâ”€â”€ main.py                       # Main execution script
â”œâ”€â”€ requirements.txt              # Project dependencies
â””â”€â”€ README.md                     # This file
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8 or higher
- Twitter Developer Account (for API access)
- Git

### Step 1: Clone the Repository

```bash
git clone https://github.com/yourusername/social-media-trend-tracker.git
cd social-media-trend-tracker
```

### Step 2: Set Up Python Environment

```bash
# Create virtual environment (recommended)
python -m venv trend_env

# Activate environment
# On Windows:
trend_env\Scripts\activate
# On Mac/Linux:
source trend_env/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### Step 3: Configure Twitter API

1. **Get Twitter API Credentials:**
   - Apply at [developer.twitter.com](https://developer.twitter.com)
   - Create a new Project and App
   - Generate API keys

2. **Set Up Configuration:**
   ```bash
   # Edit the config file
   nano config/twitter_config.ini
   ```

3. **Add your credentials:**
   ```ini
   [twitter]
   # For API v2 (Recommended)
   bearer_token = your_bearer_token_here
   
   # OR for API v1.1 (Alternative)
   # consumer_key = your_consumer_key
   # consumer_secret = your_consumer_secret  
   # access_token = your_access_token
   # access_token_secret = your_access_token_secret
   ```

### Step 4: Download NLTK Data

```python
import nltk
nltk.download('stopwords')
nltk.download('vader_lexicon')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
```

### Step 5: Run the Project

#### Option A: Complete Analysis Pipeline
```bash
python main.py
```

#### Option B: Interactive Dashboard
```bash
streamlit run dashboard/app.py
```

#### Option C: Jupyter Notebooks
```bash
jupyter notebook notebooks/
```

## ğŸ› ï¸ Features

### ğŸ“Š Data Collection
- **Twitter API Integration**: Official Tweepy integration
- **Reddit Scraping**: Pushshift API for Reddit posts
- **Real-time Streaming**: Live data collection
- **Historical Data**: Backfill historical trends

### ğŸ§¹ Data Processing
- **Text Cleaning**: URL removal, tokenization, lemmatization
- **Noise Reduction**: Stopword removal, special character handling
- **Feature Extraction**: Hashtag analysis, mention tracking
- **Quality Metrics**: Text statistics and quality assessment

### ğŸ§  NLP & Machine Learning
- **Sentiment Analysis**: VADER and Transformers-based sentiment detection
- **Topic Modeling**: LDA and NMF for theme discovery
- **Emotion Detection**: Joy, anger, sadness, fear, surprise classification
- **Trend Detection**: Identify emerging topics

### ğŸ“ˆ Visualization
- **Interactive Dashboards**: Real-time Streamlit interface
- **Time Series Analysis**: Sentiment trends over time
- **Topic Evolution**: Track theme popularity changes
- **Word Clouds**: Visual representation of frequent terms
- **Engagement Metrics**: Likes, retweets, comments analysis

## ğŸ® Usage Examples

### Basic Twitter Analysis
```python
from src.data_collection import DataCollector
from src.nlp_analysis import NLPAnalyzer

# Collect data
collector = DataCollector()
tweets = collector.scrape_twitter_data(
    queries=["AI", "machine learning"],
    days_back=7,
    max_tweets_per_query=200
)

# Analyze sentiment
analyzer = NLPAnalyzer()
sentiment_results = analyzer.analyze_sentiment_vader(tweets['text'])
```

### Custom Dashboard
Modify `dashboard/app.py` to add custom filters and visualizations.

## ğŸ“Š Sample Outputs

### Generated Visualizations
- **Sentiment Timeline**: Daily sentiment scores over time
- **Topic Distribution**: Pie charts of theme prevalence
- **Engagement Heatmaps**: Activity patterns by time and topic
- **Word Clouds**: Visual frequency analysis
- **Correlation Matrices**: Relationship between metrics

### Analysis Reports
- Weekly trend summaries
- Sentiment analysis reports
- Topic evolution tracking
- Engagement insights

## ğŸ”§ Configuration

### Search Queries
Edit `config/settings.py` to modify search terms:
```python
TWITTER_QUERIES = [
    "AI OR ChatGPT OR OpenAI",
    "machine learning OR deep learning", 
    "data science OR analytics",
    "tech OR technology",
    "programming OR coding"
]
```

### Analysis Parameters
```python
NLP_SETTINGS = {
    'sentiment_threshold': 0.05,
    'num_topics': 5,
    'max_features': 1000
}
```

## ğŸ› ï¸ Troubleshooting

### Common Issues

1. **Twitter API Limits**
   ```bash
   # Error: Rate limit exceeded
   # Solution: Wait 15 minutes or upgrade API tier
   ```

2. **Missing Dependencies**
   ```bash
   # Error: Module not found
   pip install -r requirements.txt --upgrade
   ```

3. **Configuration Issues**
   ```bash
   # Error: Invalid API credentials
   # Solution: Verify credentials in config/twitter_config.ini
   ```




